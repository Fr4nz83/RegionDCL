{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02215fb0-8d98-477a-8e5a-1cf2fc750dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import Preprocess\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346b3e3e-5f46-4ee1-ba7b-ff02590fad59",
   "metadata": {},
   "source": [
    "**NOTA**: boundary, buildings e samples devono esser gia' stati proiettati in un CRS metrico. Usare epsg=2154 (CRS metrico) per la Francia!\n",
    "\n",
    "#### Instantiating the Preprocess class...OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f74de63-4129-4983-ae5e-a9d28ba9eafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocess(\"Paris\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9e5822-4e8d-437b-a086-e1abfd375fc1",
   "metadata": {},
   "source": [
    "#### Processing the raw buildings and POIs downloaded from Geofabrik...OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a11e0b-020d-4e22-82e8-fd4222ecf39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "building, poi = preprocessor.get_building_and_poi()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602d1969-19ee-41ec-bd17-b5a27aa0f519",
   "metadata": {},
   "source": [
    "#### Performing Poisson Disk Sampling...OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1cdc71-0e7b-4c52-9e8a-486d0e7962d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "radius = 50\n",
    "random_point = preprocessor.poisson_disk_sampling(building, poi, radius)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef25774-2ae5-4ba4-a66d-3add489ebb9b",
   "metadata": {},
   "source": [
    "#### Rasterize buildings...OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323f9a9b-6904-4285-a21a-41904b3dc864",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.rasterize_buildings(building)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bca2fc-4f55-425c-b9de-ebee5253dd45",
   "metadata": {},
   "source": [
    "#### Partition city data by road network...OK\n",
    "\n",
    "**TODO**: gli autori segmentano una citta' tramite le traffic analysis zones. Problema: non ci sono shapefiles disponibili per Parigi. Per ora usiamo IRIS al posto delle TAZ, hanno densita' comparabile ed i limiti delle celle IRIS seguono la road network (analogamente alle celle TAZ)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b9bd15-5084-4b26-9b42-732ff343aecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.partition(building, poi, random_point, radius)\n",
    "print(f'Random Points: {len(random_point)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798405e4-3926-411f-913d-40629d37b37d",
   "metadata": {},
   "source": [
    "#### Chunk a previously created hdf5 file (execute only if necessary!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01309225-6cc0-4338-9b74-1fc02b83db76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Path to the original HDF5 file\n",
    "original_file_path = './data/processed/Paris/building_raster.hdf5'\n",
    "# Path to the new HDF5 file to store the chunked dataset\n",
    "new_file_path = './data/processed/Paris/building_raster_chunked.hdf5'\n",
    "\n",
    "# Open your HDF5 file\n",
    "with h5py.File(original_file_path, 'r+') as file:\n",
    "    # Access the existing dataset (adjust 'dataset_name' as needed)\n",
    "    old_dataset = file['images']\n",
    "\n",
    "    # Open a new HDF5 file in write mode to store the chunked dataset\n",
    "    with h5py.File(new_file_path, 'w') as new_file:\n",
    "\n",
    "        # Set the size of the chunks in the new dataset.\n",
    "        chunks = (128, old_dataset.shape[1], old_dataset.shape[2])\n",
    "        \n",
    "        print(f\"Number of elements in the dataset: {len(old_dataset)}\")\n",
    "        print(f\"Size of a single dataset element: {old_dataset[0].shape}\")\n",
    "        print(f\"Chunk size: {chunks}\")\n",
    "        new_dataset = new_file.create_dataset('images',\n",
    "                                              shape=old_dataset.shape,\n",
    "                                              dtype=old_dataset.dtype,\n",
    "                                              chunks=chunks,\n",
    "                                              compression=\"gzip\")  # Example: chunk size for a batch of images\n",
    "    \n",
    "        # Copy data from the old dataset to the new dataset\n",
    "        # Here assuming that loading the entire dataset into memory is feasible; otherwise, do this in smaller parts\n",
    "        dataset_size = len(old_dataset)\n",
    "        batch_size = 50000\n",
    "        num_batches = dataset_size / batch_size\n",
    "        for idx in range(0, dataset_size, batch_size) :\n",
    "            end_index = min(idx + batch_size, dataset_size)\n",
    "            new_dataset[idx : end_index] = old_dataset[idx : end_index]\n",
    "            print(f\"Copied batch from {idx} to {end_index}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
